{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiuSUz45MfwoeNJT7/eyii",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mounish2023/Build-LLMFromScratch/blob/main/reproduce_gpt_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKu4TdDKuBD3"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_hf = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "sd_hf = model_hf.state_dict()\n",
        "\n",
        "for k, v in sd_hf.items():\n",
        "    print(k,v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpWILS2-uLD8",
        "outputId": "24e6a1a0-18af-4955-cda0-0b5bd8395942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.wte.weight torch.Size([50257, 768])\n",
            "transformer.wpe.weight torch.Size([1024, 768])\n",
            "transformer.h.0.ln_1.weight torch.Size([768])\n",
            "transformer.h.0.ln_1.bias torch.Size([768])\n",
            "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.0.ln_2.weight torch.Size([768])\n",
            "transformer.h.0.ln_2.bias torch.Size([768])\n",
            "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_1.weight torch.Size([768])\n",
            "transformer.h.1.ln_1.bias torch.Size([768])\n",
            "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_2.weight torch.Size([768])\n",
            "transformer.h.1.ln_2.bias torch.Size([768])\n",
            "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.2.ln_1.weight torch.Size([768])\n",
            "transformer.h.2.ln_1.bias torch.Size([768])\n",
            "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.2.ln_2.weight torch.Size([768])\n",
            "transformer.h.2.ln_2.bias torch.Size([768])\n",
            "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.3.ln_1.weight torch.Size([768])\n",
            "transformer.h.3.ln_1.bias torch.Size([768])\n",
            "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.3.ln_2.weight torch.Size([768])\n",
            "transformer.h.3.ln_2.bias torch.Size([768])\n",
            "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.4.ln_1.weight torch.Size([768])\n",
            "transformer.h.4.ln_1.bias torch.Size([768])\n",
            "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.4.ln_2.weight torch.Size([768])\n",
            "transformer.h.4.ln_2.bias torch.Size([768])\n",
            "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.5.ln_1.weight torch.Size([768])\n",
            "transformer.h.5.ln_1.bias torch.Size([768])\n",
            "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.5.ln_2.weight torch.Size([768])\n",
            "transformer.h.5.ln_2.bias torch.Size([768])\n",
            "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.6.ln_1.weight torch.Size([768])\n",
            "transformer.h.6.ln_1.bias torch.Size([768])\n",
            "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.6.ln_2.weight torch.Size([768])\n",
            "transformer.h.6.ln_2.bias torch.Size([768])\n",
            "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.7.ln_1.weight torch.Size([768])\n",
            "transformer.h.7.ln_1.bias torch.Size([768])\n",
            "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.7.ln_2.weight torch.Size([768])\n",
            "transformer.h.7.ln_2.bias torch.Size([768])\n",
            "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.8.ln_1.weight torch.Size([768])\n",
            "transformer.h.8.ln_1.bias torch.Size([768])\n",
            "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.8.ln_2.weight torch.Size([768])\n",
            "transformer.h.8.ln_2.bias torch.Size([768])\n",
            "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.9.ln_1.weight torch.Size([768])\n",
            "transformer.h.9.ln_1.bias torch.Size([768])\n",
            "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.9.ln_2.weight torch.Size([768])\n",
            "transformer.h.9.ln_2.bias torch.Size([768])\n",
            "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.10.ln_1.weight torch.Size([768])\n",
            "transformer.h.10.ln_1.bias torch.Size([768])\n",
            "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.10.ln_2.weight torch.Size([768])\n",
            "transformer.h.10.ln_2.bias torch.Size([768])\n",
            "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.11.ln_1.weight torch.Size([768])\n",
            "transformer.h.11.ln_1.bias torch.Size([768])\n",
            "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.11.ln_2.weight torch.Size([768])\n",
            "transformer.h.11.ln_2.bias torch.Size([768])\n",
            "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.ln_f.weight torch.Size([768])\n",
            "transformer.ln_f.bias torch.Size([768])\n",
            "lm_head.weight torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sd_hf[\"transformer.wpe.weight\"].view(-1)[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R2GjWuQv_JR",
        "outputId": "98bab7b1-9c63-4d39-caaf-c0661c54b4b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0188, -0.1974,  0.0040,  0.0113,  0.0638, -0.1050,  0.0369, -0.1680,\n",
              "        -0.0491, -0.0565, -0.0025,  0.0135, -0.0042,  0.0151,  0.0166, -0.1381,\n",
              "        -0.0063, -0.0461,  0.0267, -0.2042])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sd_hf[\"lm_head.weight\"].view(-1)[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG_9sDUgCVBJ",
        "outputId": "7313d7ab-2d4b-451b-d986-8aad646257ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.1101, -0.0393,  0.0331,  0.1338, -0.0485, -0.0789, -0.2398, -0.0895,\n",
              "         0.0253, -0.1074, -0.1811, -0.0672,  0.0739, -0.0161,  0.0117,  0.1245,\n",
              "        -0.0020, -0.0815,  0.0338,  0.2365])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # from ast import increment_lineno\n",
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "# plt.imshow(sd_hf[\"transformer.wpe.weight\"], cmap=\"gray\")"
      ],
      "metadata": {
        "id": "lrUrDH1iDEwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(sd_hf[\"transformer.wpe.weight\"][:,150])\n",
        "# plt.plot(sd_hf[\"transformer.wpe.weight\"][:,200])\n",
        "# plt.plot(sd_hf[\"transformer.wpe.weight\"][:,250])"
      ],
      "metadata": {
        "id": "R-bOrtRhDm5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2\n",
        "\n",
        "1.  From the original transformer model, the encoder is removed, the middle self-attention + layernorm + residual connections are removed. the cross attention between the encoder and this middle layer of the block is not present in the gpt-2 layer.\n",
        "2.  reshuffling of layernorms (it was moved to the input of each sub-block), similar to a pre-activation residual network\n",
        "3. Additional layer normalization was added after the final self-attention block\n",
        "\n",
        "In the original transformer model from the Attention all you need paper, pe are fixed. to sinosuids and cosines of different frequencies. but in gpt2 paper they are learned."
      ],
      "metadata": {
        "id": "kPYeWIesECXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(sd_hf[\"transformer.h.1.attn.c_attn.weight\"][:300,:300], cmap='gray')"
      ],
      "metadata": {
        "id": "GywPF-FsD-Du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline, set_seed\n",
        "# generator = pipeline('text-generation',model='gpt2')\n",
        "# set_seed(42)\n",
        "# generator(text_inputs=\"Hi\",max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "id": "a1C6dDriFtWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why multihead-attention and not singlehead\n",
        "\n",
        "1. Computation is little easier on the gpu because in single head self-attention - you will have bigger Q,K,V and producing the output tensor for each token is compute heavy and moreover gpu's are good at parallel processing and we can divide these computations and concat them together at  the end.\n",
        "\n",
        "\n",
        "# Is it better in terms of model performance, is it an optimization? Is it helping in generalization with some noise inducing.\n",
        "\n",
        "The main reason why multihead self-attention exists is that each head learns a different subspace of attention. some might concentrate on the positions, some on punctuations, some on noun-verb referencing etc.,\n",
        "\n",
        "It is not noise inducing like say dropout, but these reduce overfitting and act as a regularization techniques since they are learning diversed specialized patterns.\n",
        "\n",
        "But i have this question - how can you be so sure it is learning different things and not the same thing multiple times. and also lets say we have kept a seed for the intialization at the start of training script. does this not mean the linear layers of Q,K,V are initialized the same way and will produce the same thing even when divided among different heads? explain in detail step by step. and help build intuition towards this.\n",
        "\n",
        "1. firstly, even though we kept a same seed these objects in the heads are initialized independtly and not same unless the values are actually copied.\n",
        "2. during training these subspaces of multiheads learn different things because of the gradients.\n",
        "3. it is possible sometimes some of these heads tend to concentrate on certain aspects of the semantics and actually a wasted parameters. so head pruning techniques are also getting adopted.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HXHGBw-VLm36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Multilevel perceptron is same as Feedforward layer\n",
        "2. remember the gradients are additive in nature and it is preferable to have free flow of these gradients from the supervision to the inputs. this residual pathway is important. so in the Block Module, we have additive for x\n",
        "\n",
        "\n",
        "1. Attention is an aggregation function, it is pooling function, weighted function, its a reducer function\n",
        "2. MLP or FFN (Feedforward Network) is on each individual token. they map it."
      ],
      "metadata": {
        "id": "jQ2z5CSEa9YT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why GELU and not RELU for the gpt2\n",
        "\n",
        "# because of Dead ReLU neuron problem.\n",
        "the activations that are falling near zero at the flat layer does not give any gradients. but GELU always gives out certain gradients, local ones. And makes the NNs keep going.\n",
        "\n",
        "BERT, gpt2, llama models picked this up.\n",
        "\n",
        "gpt2 uses tanh approximation of GELU non-linearity function\n",
        "\n"
      ],
      "metadata": {
        "id": "GGkW17wuJud3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from dataclasses import dataclass\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.nn import functional as F\n",
        "# import math\n",
        "\n",
        "\n",
        "\n",
        "# class CausalSelfAttention(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super().__init__()\n",
        "#         self.config = config\n",
        "#         # Rather than having three separate layers for q,k,v - we have a single layer for efficiency\n",
        "#         self.c_attn = nn.Linear(config.n_embd, 3* config.n_embd) #it is to make the x (B,T,C) into (B,T,3C)\n",
        "#         self.n_head = config.n_head\n",
        "#         self.n_embd = config.n_embd\n",
        "#         self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "#         self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "#                              .view(1,1,config.block_size, config.block_size))\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B,T,C = x.shape\n",
        "\n",
        "#         qkv = self.c_attn(x)\n",
        "#         q,k,v = qkv.split(self.n_embd, dim=2) #break them up on dim=2 which is channels (embedding dimensions)\n",
        "#         q = q.view(B,T, self.n_head, C//self.n_head ).transpose(1,2) #(B,nh,T,hs)\n",
        "#         k = k.view(B,T, self.n_head, C//self.n_head ).transpose(1,2) #(B,nh,T,hs)\n",
        "#         v = v.view(B,T, self.n_head, C//self.n_head ).transpose(1,2) #(B,nh,T,hs)\n",
        "\n",
        "#         att = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size(-1))) #computing attention scores (affinity) (B,nh,T,T)\n",
        "#         att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "#         att = F.softmax(att, dim=-1)\n",
        "#         y = att @ v #(B,nh,T,T) @ (B,nh,T,hs) -> (B,nh,T,hs)\n",
        "#         y = y.transpose(1,2).contiguous().view(B,T,C) #reassign all head outputs side by side\n",
        "#         y = self.c_proj(y)\n",
        "#         return y\n",
        "\n",
        "\n",
        "\n",
        "# class MLP(nn.Module): #Multi-layer perceptron is same as feedforward network\n",
        "#     def __init__(self, config):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd) #c_fc — \"Fully Connected\"\n",
        "#         self.gelu = nn.GELU(approximate='tanh')\n",
        "#         self.c_proj = nn.Linear(4*config.n_embd, config.n_embd)  # c_proj — \"Projection\"\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.c_fc(x)\n",
        "#         x=self.gelu(x)\n",
        "#         x = self.c_proj(x)\n",
        "#         return x\n",
        "\n",
        "# # class MLP(nn.Module): #Multi-layer perceptron is same as feedforward network\n",
        "# #     def __init__(self, config):\n",
        "# #         super().__init__()\n",
        "\n",
        "# #         self.mlp = nn.Sequential(\n",
        "# #             nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "# #             nn.GELU(approximate='tanh'),\n",
        "# #             nn.Linear(4*config.n_embd, config.n_embd)\n",
        "# #         )\n",
        "\n",
        "\n",
        "# #     def forward(self, x):\n",
        "# #         x = self.mlp(x)\n",
        "# #         return x\n",
        "\n",
        "\n",
        "# class Block(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "#         self.attn = self.MultiHeadAttention(config)\n",
        "#         self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "#         self.mlp = MLP(config)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + self.attn(self.ln_1(x))\n",
        "#         x = x + self.mlp(self.ln_2(x))\n",
        "\n",
        "#         return x\n",
        "\n",
        "# @dataclass\n",
        "# class GPTConfig:\n",
        "#     n_embd: int= 768\n",
        "#     n_layer: int= 12\n",
        "#     block_size: int = 1024\n",
        "#     batch_size: int = 64\n",
        "#     n_head: int = 12  # MultiHead attention layer. each head has different head sizes - n_embd/n_heads\n",
        "#     vocab_size: int = 50257\n",
        "\n",
        "\n",
        "# class GPT(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super().__init__()\n",
        "#         self.config = config\n",
        "\n",
        "#         self.transformer = nn.ModuleDict(\n",
        "#             dict(\n",
        "#                 wte = nn.Embedding(config.vocab_size, config.n_embd ),\n",
        "#                 wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "#                 h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "#                 ln_f = nn.LayerNorm(config.n_embd)\n",
        "#             )\n",
        "#         )\n",
        "#         self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias= False)\n",
        "\n",
        "#     def  forward(self, idx):\n",
        "#         B,T = idx.shape\n",
        "#         assert T <=self.config.block_size, f\"Cannot forward sequence of length {T}, block size is {self.config.block_size}\"\n",
        "\n",
        "#         pos = torch.arange(0,T, dtype=torch.long, device=idx.device)\n",
        "#         pos_emb = self.transformer.wpe(pos)\n",
        "#         tok_emb = self.transformer.wte(idx)\n",
        "\n",
        "#         x = tok_emb + pos_emb\n",
        "\n",
        "#         for block in self.transformer.h:\n",
        "#             x = block(x)\n",
        "\n",
        "#         x = self.transformer.ln_f(x)\n",
        "\n",
        "#         logits = self.lm_head(x)\n",
        "#         return logits\n",
        "\n",
        "#     @classmethod\n",
        "#     def from_pretrained(cls, model_type):\n",
        "#         \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "#         assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "#         from transformers import GPT2LMHeadModel\n",
        "#         print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "#         # n_layer, n_head and n_embd are determined from model_type\n",
        "#         config_args = {\n",
        "#             'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "#             'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "#             'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "#             'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "#         }[model_type]\n",
        "#         config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "#         config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "#         # create a from-scratch initialized minGPT model\n",
        "#         config = GPTConfig(**config_args)\n",
        "#         model = GPT(config)\n",
        "#         sd = model.state_dict()\n",
        "#         sd_keys = sd.keys()\n",
        "#         sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "#         # init a huggingface/transformers model\n",
        "#         model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "#         sd_hf = model_hf.state_dict()\n",
        "\n",
        "#         # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "#         sd_keys_hf = sd_hf.keys()\n",
        "#         sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "#         sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "#         transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "#         # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "#         # this means that we have to transpose these weights when we import them\n",
        "#         assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "#         for k in sd_keys_hf:\n",
        "#             if any(k.endswith(w) for w in transposed):\n",
        "#                 # special treatment for the Conv1D weights we need to transpose\n",
        "#                 assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "#                 with torch.no_grad():\n",
        "#                     sd[k].copy_(sd_hf[k].t())\n",
        "#             else:\n",
        "#                 # vanilla copy over the other parameters\n",
        "#                 assert sd_hf[k].shape == sd[k].shape\n",
        "#                 with torch.no_grad():\n",
        "#                     sd[k].copy_(sd_hf[k])\n",
        "\n",
        "#         return model\n",
        "\n",
        "# # num_return_sequences = 5\n",
        "# # max_length = 30\n",
        "\n",
        "# # model = GPT.from_pretrained('gpt2')\n",
        "# # print(\"didn't crash yay!\")\n",
        "\n",
        "# # model.eval()\n",
        "# # model.to('cuda')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pCpfWbCLG34J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Created Model classes to import the Model parameters of gpt2\n",
        "# Created DataLoaders that provides next batches for training.\n",
        "#\n",
        "\n",
        "# Tricky things\n",
        "1. Weights initialization: Xavier std - 1/sqrt(dimension)\n",
        "    a. Linear layers - bias\n",
        "    b. Embedding layers\n",
        "    c. Residual connections\n",
        "    d. LayerNorm\n",
        "\n",
        "Xavier initialization, also known as Glorot initialization, is a widely used method for initializing the weights of neural networks to help prevent the problems of vanishing or exploding gradients during training.\n",
        "\n",
        "scale the activations to make sure that the std are within the range of 1 in the residual pathways during the forward pass. this scaling is done with 1/sqrt(N) (2 * n_layers).\n",
        "\n",
        "2. Layer Normalization in transformers (across columns) / BatchNorm in other things (across samples)\n",
        "3. weights of token embeddings and output linear head are same.\n",
        "4. CausalSelfAttention - batches, number of heads, T, C (head size)"
      ],
      "metadata": {
        "id": "aNWz2-4rT6bJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel fusion. model = torch.compile(model)\n",
        "\n",
        "takes all of the code and makes it into one object for computation"
      ],
      "metadata": {
        "id": "sdBsyH64n4oE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Increasing the speed of training.\n",
        "\n",
        "1. reduce mantissa (precision) of the floating point numbers\n",
        "FP32 -> TF32 ->BF16\n",
        "if we go for FP16 -> we have to implement gradient scalers. for others, there is very little code change.\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "2. TF32 ->BF16 (pytorch converts some of the matrices to bfloat16, but some of it are still left at float32)\n",
        "     \n",
        "     with torch.autocast(device_type=device, dtype = torch.bfloat16):\n",
        "        logits, loss = model(x,y)\n",
        "the bottleneck here is still the memory. even though operations are happening very fast, these numbers need to be accessed and transported. it is time consuming process.\n",
        "\n",
        "3. torch.compile(model)\n",
        "with the benefit of knowing what are the operations that are going to happen before hand, it can reduce the number of roundtrips that need to happen between GPU and HBM. there is a memory hierarchical structure that needs to be understood to make the analysis better.\n",
        "\n",
        "but even torch.compile doesn't do all of the operations in GPU - for example self attention layers.\n",
        "\n",
        "4. Flash attention - all of the multiheaded causal self-attention operations are done with the help of kernel fusion.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "91H-I0Tcqzi9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VcaOAPDGjA_4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}