{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOz925wlY6bRWj8EWVWUjL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mounish2023/Build-LLMFromScratch/blob/main/gpt2_writeup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loss function - cross entropy\n",
        "\n",
        "Cross-entropy is a commonly used loss function in classification problems. It measures how well the predicted probability distribution aligns with the true distribution (the ground truth labels). A lower cross-entropy value indicates better model performance on the training data. However, consistently low training loss without generalization to validation data can indicate overfitting.\n",
        "\n",
        "Cross-entropy is widely used in both binary and multiclass classification. In multiclass classification, it helps the model learn to assign high probability to the correct class among many possible classes. It is especially prevalent in NLP tasks such as language modeling, text classification, and machine translation, where models must choose from a large vocabulary.\n",
        "\n",
        "In multiclass classification, cross-entropy loss combines the softmax function (which turns logits into probabilities) with the negative log-likelihood (which penalizes incorrect predictions). This is often implemented as log_softmax + NLLLoss for numerical stability. While softmax is used for multi-class problems, sigmoid is used in binary classification or multi-label classification.\n",
        "\n",
        "So why is this used to train models?\n",
        "The goal of training a model is to minimize its error — or make it \"less wrong\" — across both the training data and unseen data. Cross-entropy loss quantifies this error in classification tasks by measuring how far off the predicted probability distribution is from the true distribution (which is typically one-hot encoded).\n",
        "\n",
        "\n",
        "softmax converts logits to probabilities. it is applied over a sequence of elements and normalizes them such that sum of the probabilities of all elements in the sequence would be 1. So with softmax, you get the probability distribution of the output categories. but there is an issue with softmax, it makes the differences in logits very stark.\n",
        "\n",
        "Softmax highlights the largest logits by assigning them higher probabilities. This helps models become confident in predictions, but can also make them overly confident or numerically unstable if logits are very large. To address this, techniques like temperature scaling or log-softmax are used.\n",
        "\n",
        "logarithms are important in the deep learning as it makes sure that growth is linear and not exponential. Logarithms are used in the loss function to penalize incorrect predictions more strongly. For example, if the model predicts a very low probability for the correct class, the negative log will result in a high loss. This helps guide learning. Also, logarithmic functions help compress large ranges of values and are mathematically well-suited for measuring divergence between probability distributions (as in entropy).\n",
        "\n",
        "The cross-entropy loss is calculated as the negative log of the predicted probability for the correct class. For a batch of samples, this is averaged across all examples. The model is trained to minimize this loss — i.e., maximize the probability assigned to the correct class."
      ],
      "metadata": {
        "id": "5eqUyB0q2x-C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEOKREKP2sub"
      },
      "outputs": [],
      "source": []
    }
  ]
}